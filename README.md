# Queue Reinforcement Learning (QRL)

## Overview
This project implements simulation and reinforcement learning for queueing systems, with a focus on dynamically controlling service rates and resource allocation. It provides tools for modeling M/M/1 and M/M/c queues, simulating their behavior, and applying reinforcement learning to optimize their performance.

## TODO
- [] training pipeline + script
- [] model eval script
- [] visualization pipline + interactive experiement env
- [] baselines
- [] (potentially) RLHF based real time rate tuning and adjustment models

## Mathematical Background

### Queueing Theory Fundamentals

#### Markov Processes
Queueing systems are typically modeled as continuous-time Markov chains (CTMCs). A CTMC is a stochastic process {X(t), t ‚â• 0} with the Markov property:

P(X(t+s) = j | X(s) = i, X(u) = x(u), 0 ‚â§ u < s) = P(X(t+s) = j | X(s) = i)

for all s, t ‚â• 0 and states i, j. This memoryless property is fundamental to queueing models with exponential interarrival and service times.

#### Birth-Death Processes
Birth-death processes are a special case of CTMCs where transitions only occur between adjacent states. The state typically represents the number of customers in the system.

The infinitesimal generator matrix Q of a birth-death process has the form:
```
    | -Œª‚ÇÄ   Œª‚ÇÄ    0     0     ... |
    | Œº‚ÇÅ   -(Œª‚ÇÅ+Œº‚ÇÅ)  Œª‚ÇÅ    0     ... |
Q = | 0     Œº‚ÇÇ   -(Œª‚ÇÇ+Œº‚ÇÇ)  Œª‚ÇÇ    ... |
    | 0     0     Œº‚ÇÉ   -(Œª‚ÇÉ+Œº‚ÇÉ)  ... |
    | ...   ...   ...   ...    ... |
```

where Œª‚Çô is the birth rate (arrival rate) when in state n, and Œº‚Çô is the death rate (service rate) when in state n.

### M/M/1 Queue

An M/M/1 queue has:
- Poisson arrivals (rate Œª)
- Exponential service times (rate Œº)
- Single server
- First-come, first-served (FCFS) discipline

Key performance metrics for M/M/1:

- Traffic intensity (utilization): œÅ = Œª/Œº
- Steady-state probability of n customers: œÄ‚Çç‚Çô‚Çé = (1-œÅ)œÅ‚Åø for œÅ < 1
- Mean number of customers: L = œÅ/(1-œÅ)
- Mean waiting time: W = 1/(Œº-Œª)
- Mean queue length: L_q = œÅ¬≤/(1-œÅ)
- Mean time in queue: W_q = œÅ/(Œº-Œª)

### M/M/c Queue

An M/M/c queue has:
- Poisson arrivals (rate Œª)
- Exponential service times (rate Œº per server)
- c identical servers in parallel
- FCFS discipline

Key performance metrics for M/M/c:

- Traffic intensity: œÅ = Œª/(cŒº)
- Erlang's C formula (probability of waiting): C(c,œÅ) = ((cœÅ)·∂ú/c!)/(‚àë‚Çñ‚Çå‚ÇÄ·∂ú‚Åª¬π(cœÅ)·µè/k! + (cœÅ)·∂ú/c!(1-œÅ))
- Mean number of customers: L = cœÅ + C(c,œÅ)¬∑œÅ/(1-œÅ)
- Mean waiting time: W = 1/Œº + C(c,œÅ)/(cŒº-Œª)
- Mean queue length: L_q = C(c,œÅ)¬∑œÅ/(1-œÅ)
- Mean time in queue: W_q = C(c,œÅ)/(cŒº-Œª)

### Gillespie Algorithm

The Gillespie algorithm (also known as the Stochastic Simulation Algorithm or SSA) is used to simulate the exact trajectory of CTMCs:

1. Calculate the total transition rate q = ‚àë·µ¢‚±ºq·µ¢‚±º where i is the current state
2. Sample the time until the next event from an exponential distribution with rate q: Œît ~ Exp(q)
3. Sample the next state j with probability q·µ¢‚±º/q
4. Update the time t = t + Œît and state i = j
5. Repeat until the end condition is met

## Reinforcement Learning for Queue Control

### Markov Decision Process Formulation

Queue control can be formulated as a Markov Decision Process (MDP):
- State space: Queue length, server states, etc.
- Action space: Service rate adjustment, number of servers, etc.
- Transition function: Defined by the queueing dynamics
- Reward function: Typically a combination of throughput, waiting time, and resource costs

### Value Functions and Bellman Equations

The state-value function V(s) represents the expected return starting from state s:

V(s) = ùîº[‚àë·µè‚Çå‚ÇÄ^‚àû Œ≥·µè¬∑r‚Çñ‚Çä‚ÇÅ | S‚ÇÄ = s]

The action-value function Q(s,a) represents the expected return after taking action a in state s:

Q(s,a) = ùîº[‚àë·µè‚Çå‚ÇÄ^‚àû Œ≥·µè¬∑r‚Çñ‚Çä‚ÇÅ | S‚ÇÄ = s, A‚ÇÄ = a]

Bellman equations relate the value of a state to the values of its successor states:

V(s) = ‚àë‚Çê œÄ(a|s) ‚àë‚Çõ‚Ä≤ p(s‚Ä≤|s,a)[r(s,a,s‚Ä≤) + Œ≥V(s‚Ä≤)]
Q(s,a) = ‚àë‚Çõ‚Ä≤ p(s‚Ä≤|s,a)[r(s,a,s‚Ä≤) + Œ≥‚àë‚Çê‚Ä≤ œÄ(a‚Ä≤|s‚Ä≤)Q(s‚Ä≤,a‚Ä≤)]

### Reinforcement Learning Algorithms

Several RL algorithms can be applied to queue control:

1. **Q-learning**: Model-free algorithm that learns the action-value function:
   Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥¬∑max_a‚Ä≤ Q(s‚Ä≤,a‚Ä≤) - Q(s,a)]

2. **SARSA**: On-policy algorithm that updates based on the next action taken:
   Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥¬∑Q(s‚Ä≤,a‚Ä≤) - Q(s,a)]

3. **Policy Gradient**: Directly optimizes the policy parameters Œ∏:
   Œ∏ ‚Üê Œ∏ + Œ±¬∑‚àálog œÄ(a|s;Œ∏)¬∑G_t

4. **Actor-Critic**: Combines value function approximation and policy optimization:
   Œ∏ ‚Üê Œ∏ + Œ±¬∑‚àálog œÄ(a|s;Œ∏)¬∑Œ¥
   where Œ¥ = r + Œ≥¬∑V(s‚Ä≤) - V(s) is the temporal difference error

## Optimization Objectives

Common optimization objectives in queue control include:

1. **Minimizing average waiting time**: min E[W]
2. **Maximizing throughput**: max Œª_eff (effective arrival rate)
3. **Minimizing resource costs**: min c¬∑Œº (service cost) or min n¬∑c_s (server cost)
4. **Multi-objective optimization**: min w‚ÇÅ¬∑E[W] + w‚ÇÇ¬∑cost, where w‚ÇÅ and w‚ÇÇ are weights

## Example Policies

1. **Threshold Policy**: Increase service rate when queue length exceeds a threshold
   Œº(t) = Œº‚ÇÅ if q(t) ‚â§ K, Œº‚ÇÇ otherwise (Œº‚ÇÇ > Œº‚ÇÅ)

2. **Linear Control Policy**: Service rate proportional to queue length
   Œº(t) = Œº‚ÇÄ + Œ±¬∑q(t)

3. **N-Policy**: Only serve when queue length reaches N customers
   serve if q(t) ‚â• N, idle otherwise

4. **Dynamic Server Allocation**: Adjust number of servers based on queue length
   c(t) = min(max(‚åàq(t)/K‚åâ, c_min), c_max)

## References

1. Kleinrock, L. (1975). Queueing Systems, Volume 1: Theory.
2. Tijms, H.C. (2003). A First Course in Stochastic Models.
3. Sutton, R.S., & Barto, A.G. (2018). Reinforcement Learning: An Introduction.
4. Puterman, M.L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming.
5. Gillespie, D.T. (1977). Exact stochastic simulation of coupled chemical reactions.

## Installation and Usage

```bash
# Install dependencies
pip install -r requirements.txt

# Run M/M/1 queue simulation
python src/main.py mm1 --arrival-rate 2.0 --service-rate 3.0

# Run M/M/c queue simulation
python src/main.py mmc --arrival-rate 5.0 --service-rate 1.5 --num-servers 4

# Run tests
python src/main.py test
``` 
